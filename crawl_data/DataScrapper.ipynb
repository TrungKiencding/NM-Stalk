{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a816ec3f",
   "metadata": {},
   "source": [
    "# I, Import librabries\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d80079a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import AzureChatOpenAI\n",
    "import requests\n",
    "from crawl4ai import AsyncWebCrawler\n",
    "from typing import List, Dict\n",
    "from pymilvus.model.sparse.bm25.tokenizers import build_default_analyzer\n",
    "from pymilvus.model.sparse import BM25EmbeddingFunction\n",
    "import scipy.sparse as sp\n",
    "from output import example_doc,duplicate\n",
    "api_key = 'GOOGLE SEARCH API KEY'\n",
    "search_engine_id = 'GOOGLE SEARCH ENGINE ID'\n",
    "from pymilvus import DataType, MilvusClient\n",
    "from pymilvus import (\n",
    "    MilvusClient,\n",
    "    DataType,\n",
    ")\n",
    "import feedparser\n",
    "from datetime import datetime, timedelta\n",
    "import time\n",
    "from bs4 import BeautifulSoup\n",
    "from datetime import timezone\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "9cc26dd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize session\n",
    "session = requests.Session()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc901aaa",
   "metadata": {},
   "source": [
    "# II, Vector Databse"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "596c4076",
   "metadata": {},
   "source": [
    "## 1, Create Collection and Schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34e8aadf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/mental_health/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collection 'mycollection' created successfully\n"
     ]
    }
   ],
   "source": [
    "uri = \"/Users/qa.nguyen/Documents/Intern/Viettel/WebScrapper/MyWebThinker/myvilvus.db\"\n",
    "collection_name = \"mycollection\"\n",
    "client = MilvusClient(uri=uri)\n",
    "from pymilvus.model.sparse.bm25.tokenizers import build_default_analyzer\n",
    "from pymilvus.model.sparse import BM25EmbeddingFunction\n",
    "analyzer = build_default_analyzer(language=\"en\")\n",
    "bm25_ef = BM25EmbeddingFunction(analyzer)\n",
    "analyzer_params = {\"tokenizer\": \"standard\", \"filter\": [\"lowercase\"]}\n",
    "schema = MilvusClient.create_schema()\n",
    "schema.add_field(\n",
    "    field_name=\"id\",\n",
    "    datatype=DataType.VARCHAR,\n",
    "    is_primary=True,\n",
    "    auto_id=True,\n",
    "    max_length=100,\n",
    ")\n",
    "schema.add_field(\n",
    "    field_name=\"content\",\n",
    "    datatype=DataType.VARCHAR,\n",
    "    max_length=65535,\n",
    "    analyzer_params=analyzer_params,\n",
    "    enable_match=True,  # Enable text matching\n",
    "    enable_analyzer=True,  # Enable text analysis\n",
    ")\n",
    "schema.add_field(\n",
    "    field_name=\"link\",\n",
    "    datatype=DataType.VARCHAR,\n",
    "    max_length=100\n",
    ")\n",
    "schema.add_field(field_name=\"sparse_vector\", datatype=DataType.SPARSE_FLOAT_VECTOR)\n",
    "index_params = MilvusClient.prepare_index_params()\n",
    "index_params.add_index(\n",
    "    field_name=\"sparse_vector\",\n",
    "    index_type=\"SPARSE_INVERTED_INDEX\",\n",
    "    metric_type=\"IP\",\n",
    ")\n",
    "if client.has_collection(collection_name):\n",
    "    client.drop_collection(collection_name)\n",
    "# Create the collection\n",
    "client.create_collection(\n",
    "    collection_name=collection_name,\n",
    "    schema=schema,\n",
    "    index_params=index_params,\n",
    ")\n",
    "\n",
    "print(f\"Collection '{collection_name}' created successfully\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ce51d59",
   "metadata": {},
   "source": [
    "## 2, Create Vector Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94c094bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorEmbedding(collection_name, docs, links):\n",
    "    \"\"\"\n",
    "    Input:\n",
    "    - collection_name: collection name for storing vector db\n",
    "    - docs: list of documents to be embedded\n",
    "    - links: list of links need to be stored into db\n",
    "    Output: no output, the function just insert data into database\n",
    "\n",
    "    \"\"\"\n",
    "    sparse_dicts = []\n",
    "    bm25_ef.fit(docs)\n",
    "    docs_embeddings = bm25_ef.encode_documents(docs)\n",
    "    for vec in docs_embeddings:\n",
    "        if sp.issparse(vec):\n",
    "            coo = vec.tocoo()\n",
    "            sparse_dicts.append({int(c): float(d) for c, d in zip(coo.col, coo.data)})\n",
    "        else:  # assume dense numpy array\n",
    "            sparse_dicts.append({i: float(v) for i, v in enumerate(vec) if v != 0.0})\n",
    "    entities = []\n",
    "\n",
    "    for i in range (len(docs)):\n",
    "        entities.append(\n",
    "            {\n",
    "                \"content\": docs[i],\n",
    "                \"link\": links[i],\n",
    "                \"sparse_vector\": sparse_dicts[i],   # <-- include your BM25 vector here\n",
    "            }\n",
    "        )\n",
    "\n",
    "    # Insert data\n",
    "    client.insert(collection_name, entities)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f55f1604",
   "metadata": {},
   "source": [
    "## 3, Remove Duplicate Links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8828cf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def remove_duplicates(links):\n",
    "    \"\"\"\n",
    "    Input: A list of links\n",
    "    Output: A list of unique links\n",
    "    \"\"\"\n",
    "    if not links:\n",
    "        return []\n",
    "    else:\n",
    "        for link in links:\n",
    "            res = client.query(\n",
    "    collection_name=\"mycollection\",\n",
    "    filter=f\"link like \\\"{link}\\\"\",\n",
    "    output_fields=[\"link\"],\n",
    "    limit=1\n",
    "            )\n",
    "            if res:\n",
    "                links.remove(link)\n",
    "    return links"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2117a90",
   "metadata": {},
   "source": [
    "## 4, Check Repost Contents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f9424aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def repost(doc):\n",
    "    return False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "025203e3",
   "metadata": {},
   "source": [
    "# III, Get Daily Papers From Arxiv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "789e6acd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_arxiv_by_subject_and_dates(\n",
    "    subject: str,\n",
    "    start: datetime,\n",
    "    end: datetime,\n",
    "    max_results: int = 50\n",
    ") -> List[Dict[str, str]]:\n",
    "    \"\"\"\n",
    "    Query arXiv for papers in `subject` whose submission times fall\n",
    "    between `start` and `end` (both datetimes in UTC).\n",
    "\n",
    "    Returns a list of {\"link\": ..., \"abstract\": ...} dicts.\n",
    "    \"\"\"\n",
    "    # Format datetimes as YYYYMMDDHHMM (GMT)\n",
    "    fmt = \"%Y%m%d%H%M\"\n",
    "    start_s = start.strftime(fmt)\n",
    "    end_s   = end.strftime(fmt)\n",
    "\n",
    "    # Build the search_query string\n",
    "    # Use '+' for spaces/AND between terms in the URL\n",
    "    date_filter = f\"submittedDate:[{start_s}+TO+{end_s}]\"\n",
    "    query = f\"cat:{subject}+AND+{date_filter}\"\n",
    "\n",
    "    # Construct the full API URL\n",
    "    base_url = \"http://export.arxiv.org/api/query?\"\n",
    "    url = (\n",
    "        f\"{base_url}\"\n",
    "        f\"search_query={query}\"\n",
    "        f\"&start=0\"\n",
    "        f\"&max_results={max_results}\"\n",
    "        f\"&sortBy=submittedDate\"\n",
    "        f\"&sortOrder=descending\"\n",
    "    )\n",
    "\n",
    "    # Fetch and parse the Atom feed\n",
    "    feed = feedparser.parse(url)\n",
    "    links = []\n",
    "    abstracts = []\n",
    "    for entry in feed.entries:\n",
    "        links.append(entry.id)\n",
    "        abstracts.append(entry.summary.strip().replace(\"\\n\", \" \"))\n",
    "    return links, abstracts\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f8ad4d1",
   "metadata": {},
   "source": [
    "# IV, Get Daily Trending Repo From Github\n",
    "- This can change to be weekly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b158ddc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "TRENDING_URL = \"https://github.com/trending?since=daily\"\n",
    "HEADERS = {\n",
    "    \"User-Agent\": (\n",
    "        \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) \"\n",
    "        \"AppleWebKit/537.36 (KHTML, like Gecko) \"\n",
    "        \"Chrome/124.0 Safari/537.36\"\n",
    "    )\n",
    "}\n",
    "\n",
    "def fetch_trending_repos(max_repos: int = 20) -> List[str]:\n",
    "    \"\"\"Return a list like [\"owner1/repo1\", \"owner2/repo2\", …].\"\"\"\n",
    "    resp = requests.get(TRENDING_URL, headers=HEADERS, timeout=15)\n",
    "    resp.raise_for_status()\n",
    "\n",
    "    soup = BeautifulSoup(resp.text, \"html.parser\")\n",
    "    links = soup.select(\"article.Box-row h2 a\")  # repo links\n",
    "    repos = []\n",
    "    for a in links[:max_repos]:\n",
    "        # href looks like \"/owner/repo\"\n",
    "        repo_path = a[\"href\"].strip(\"/\")\n",
    "        repos.append(repo_path)\n",
    "    return repos\n",
    "\n",
    "\n",
    "def grab_readme(owner_repo: str) -> Dict[str, str]:\n",
    "    \"\"\"\n",
    "    Attempt to fetch the raw README from the repo’s default branch (main/master).\n",
    "    Returns dict {\"repo\": \"<owner/repo>\", \"url\": \"<raw-url>\", \"content\": \"<md or ''>\"}\n",
    "    \"\"\"\n",
    "    owner, repo = owner_repo.split(\"/\")\n",
    "    branches = [\"main\", \"master\"]\n",
    "    content = \"\"\n",
    "    raw_url = \"\"\n",
    "    for br in branches:\n",
    "        url = f\"https://raw.githubusercontent.com/{owner}/{repo}/{br}/README.md\"\n",
    "        r = requests.get(url, headers=HEADERS, timeout=15)\n",
    "        if r.status_code == 200 and r.text.strip():\n",
    "            raw_url = url\n",
    "            content = r.text\n",
    "            break\n",
    "    return {\"repo\": owner_repo, \"url\": raw_url, \"content\": content}\n",
    "\n",
    "def getTrendingRepo():\n",
    "    top_repos = fetch_trending_repos(max_repos=10)\n",
    "    readmeLinks = []\n",
    "    readmeContent = []\n",
    "    for idx, repo in enumerate(top_repos, 1):\n",
    "        data = grab_readme(repo)\n",
    "         \n",
    "        if data[\"content\"]:\n",
    "            readmeLinks.append(data['url'])\n",
    "            readmeContent.append(data[\"content\"])\n",
    "        else:\n",
    "            print(f\"[{idx}/{len(top_repos)}] {repo}: README not found on main/master.\\n\")\n",
    "        time.sleep(1)   # polite delay\n",
    "    return readmeLinks, readmeContent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d77c3eca",
   "metadata": {},
   "source": [
    "# V, Useful Tools\n",
    "\n",
    "- web_extract_sync: Extract Website content in markdown\n",
    "- EnrichContent: Find related links and contents to the input post\n",
    "- google_search: Function to search Google by query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3beca188",
   "metadata": {},
   "outputs": [],
   "source": [
    "#google search\n",
    "def google_search(query, api_key=api_key,search_engine_id=search_engine_id, **params, ):\n",
    "    \"\"\"\n",
    "    Perform an asynchronous search using the Google Search Engine.\n",
    "\n",
    "    Args:\n",
    "        query (str): Search query.\n",
    "        api_search_key (str): key for the Google Search API.\n",
    "        search_engine_id (str): key for Google Search Engine\n",
    "    Returns:\n",
    "        dict: JSON response of the search results. Returns empty dict if all retries fail.\n",
    "    \"\"\"\n",
    "    query = query\n",
    "    base_url = 'https://www.googleapis.com/customsearch/v1'\n",
    "    params = {\n",
    "        'key': api_key,\n",
    "        'cx': search_engine_id,\n",
    "        'q': query,\n",
    "        **params\n",
    "    }\n",
    "    response = session.get(base_url, params=params)\n",
    "    response.raise_for_status\n",
    "    search_result = response.json()\n",
    "    return search_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "612e67a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def _web_extract_tool(urls: List[str]) -> List[dict]:\n",
    "    \"\"\"\n",
    "    Async content‐extraction using crawl4ai. \n",
    "    Returns a list of dicts: {'link': <url>, 'raw_content': <markdown>}.\n",
    "    \"\"\"\n",
    "    useful_info = []\n",
    "    # We can reuse one single AsyncWebCrawler per URL to keep things simple.\n",
    "    async with AsyncWebCrawler() as crawler:\n",
    "            results = await crawler.arun_many(urls)\n",
    "            for result in results:\n",
    "                if result.success:\n",
    "                    # Print to console so you see progress\n",
    "                    useful_info.append(result.markdown)\n",
    "                else:\n",
    "                    # If crawl failed, still record the link with an error message\n",
    "                    useful_info.append({\n",
    "                        \"link\": result.url,\n",
    "                        \"raw_content\": f\"[FAILED to crawl]\"\n",
    "                    })\n",
    "    return useful_info\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f821e0c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "def web_extract_sync(urls: List[str]) -> List[dict]:\n",
    "    \"\"\"\n",
    "    Synchronous wrapper for _web_extract_tool.\n",
    "    Internally runs the async function on an event loop and returns List[dict].\n",
    "    \"\"\"\n",
    "    return asyncio.run(_web_extract_tool(urls))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6d14434",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "llm = AzureChatOpenAI(\n",
    "    azure_endpoint=\"\",\n",
    "    api_key=\"\",\n",
    "    api_version=\"\",\n",
    "    deployment_name=\"gpt-4o\",\n",
    "    temperature=0.2,   # lower temperature to minimize hallucinations\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05523b90",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "template = \"\"\"\n",
    "You’re given a piece of text describing a project, paper, or tool.  \n",
    "Your job is to **extract the one most important keyword or name** that a user could plug into Google Scholar to find broad, relevant information.  \n",
    "\n",
    "– Only output a single word or short phrase (1–5 words).  \n",
    "– Do not include quotes, punctuation, or any explanation.  \n",
    "– Make it as general as possible (e.g. “WebThinker” rather than “WebThinker deep research framework…”).\n",
    "\n",
    "### Examples\n",
    "\n",
    "Text:\n",
    "“WebThinker: Empowering Large Reasoning Models with Deep Research Capability. … The code is available at https://github.com/RUC-NLPIR/WebThinker.”\n",
    "\n",
    "Output:\n",
    "WebThinker\n",
    "\n",
    "Text:\n",
    "“In recent years particle filters have been used for robot localization… we implement a hybrid landmark‐and‐wall‐based Bayesian filter in Webots.”\n",
    "\n",
    "Output:\n",
    "Particle filter localization\n",
    "\n",
    "---\n",
    "\n",
    "Text:\n",
    "{post}\n",
    "\n",
    "Output:\n",
    "\"\"\"\n",
    "\n",
    "prompt_template = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", template),\n",
    "    (\"user\", \"{post}\")\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b58625c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def EnrichContent(docs):\n",
    "    for doc in docs:\n",
    "        prompt = prompt_template.invoke({\"post\": doc})\n",
    "        response = llm.invoke(prompt)\n",
    "        print(response.content)\n",
    "        search_results = google_search(query=query)\n",
    "        links = []\n",
    "        if 'items' in search_results:\n",
    "            for idx, item in enumerate(search_results['items'][:3]):\n",
    "                link =  item.get('link', '')\n",
    "                links.append(link)\n",
    "        links = remove_duplicates(links)\n",
    "        #links = repost(links)\n",
    "        docs = web_extract_sync(links)\n",
    "        vectorEmbedding(\"mycollection\", docs, links)\n",
    "    print(links)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "0b4cdc0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[6/10] eythaann/Seelen-UI: README not found on main/master.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Define your two-day window (UTC)\n",
    "today = datetime.now(timezone.utc)\n",
    "two_days_ago = today.replace(hour=0, minute=0, second=0, microsecond=0) \\\n",
    "                    - timedelta(days=2)\n",
    "\n",
    "ArxivLinks, ArxivAbstracts = get_arxiv_by_subject_and_dates(\n",
    "    subject=\"cs.AI\",\n",
    "    start=two_days_ago,\n",
    "    end=today,\n",
    "    max_results=10 #20\n",
    ")\n",
    "\n",
    "GitHubLinks, GithubRePo = getTrendingRepo()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "181b74ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['http://arxiv.org/abs/2506.09050v1', 'http://arxiv.org/abs/2506.09049v1', 'http://arxiv.org/abs/2506.09046v1', 'http://arxiv.org/abs/2506.09040v1', 'http://arxiv.org/abs/2506.09038v1', 'http://arxiv.org/abs/2506.09034v1', 'http://arxiv.org/abs/2506.09033v1', 'http://arxiv.org/abs/2506.09027v1', 'http://arxiv.org/abs/2506.09018v1', 'http://arxiv.org/abs/2506.08999v1']\n"
     ]
    }
   ],
   "source": [
    "print(ArxivLinks)\n",
    "print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mental_health",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
