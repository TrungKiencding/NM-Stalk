[
    {
      "link": "https://github.com/confident-ai/deepeval",
      "raw_content": "[Skip to content](https://github.com/confident-ai/deepeval#start-of-content)\n## Navigation Menu\nToggle navigation\n[ ](https://github.com/)\n[ Sign in ](https://github.com/login?return_to=https%3A%2F%2Fgithub.com%2Fconfident-ai%2Fdeepeval)\nAppearance settings\n  * Product \n    * [ GitHub Copilot  Write better code with AI  ](https://github.com/features/copilot)\n    * [ GitHub Models  New  Manage and compare prompts  ](https://github.com/features/models)\n    * [ GitHub Advanced Security  Find and fix vulnerabilities  ](https://github.com/security/advanced-security)\n    * [ Actions  Automate any workflow  ](https://github.com/features/actions)\n    * [ Codespaces  Instant dev environments  ](https://github.com/features/codespaces)\n    * [ Issues  Plan and track work  ](https://github.com/features/issues)\n    * [ Code Review  Manage code changes  ](https://github.com/features/code-review)\n    * [ Discussions  Collaborate outside of code  ](https://github.com/features/discussions)\n    * [ Code Search  Find more, search less  ](https://github.com/features/code-search)\nExplore\n    * [ Why GitHub ](https://github.com/why-github)\n    * [ All features ](https://github.com/features)\n    * [ Documentation ](https://docs.github.com)\n    * [ GitHub Skills ](https://skills.github.com)\n    * [ Blog ](https://github.blog)\n  * Solutions \nBy company size\n    * [ Enterprises ](https://github.com/enterprise)\n    * [ Small and medium teams ](https://github.com/team)\n    * [ Startups ](https://github.com/enterprise/startups)\n    * [ Nonprofits ](https://github.com/solutions/industry/nonprofits)\nBy use case\n    * [ DevSecOps ](https://github.com/solutions/use-case/devsecops)\n    * [ DevOps ](https://github.com/solutions/use-case/devops)\n    * [ CI/CD ](https://github.com/solutions/use-case/ci-cd)\n    * [ View all use cases ](https://github.com/solutions/use-case)\nBy industry\n    * [ Healthcare ](https://github.com/solutions/industry/healthcare)\n    * [ Financial services ](https://github.com/solutions/industry/financial-services)\n    * [ Manufacturing ](https://github.com/solutions/industry/manufacturing)\n    * [ Government ](https://github.com/solutions/industry/government)\n    * [ View all industries ](https://github.com/solutions/industry)\n[ View all solutions ](https://github.com/solutions)\n  * Resources \nTopics\n    * [ AI ](https://github.com/resources/articles/ai)\n    * [ DevOps ](https://github.com/resources/articles/devops)\n    * [ Security ](https://github.com/resources/articles/security)\n    * [ Software Development ](https://github.com/resources/articles/software-development)\n    * [ View all ](https://github.com/resources/articles)\nExplore\n    * [ Learning Pathways ](https://resources.github.com/learn/pathways)\n    * [ Events & Webinars ](https://resources.github.com)\n    * [ Ebooks & Whitepapers ](https://github.com/resources/whitepapers)\n    * [ Customer Stories ](https://github.com/customer-stories)\n    * [ Partners ](https://partner.github.com)\n    * [ Executive Insights ](https://github.com/solutions/executive-insights)\n  * Open Source \n    * [ GitHub Sponsors  Fund open source developers  ](https://github.com/sponsors)\n    * [ The ReadME Project  GitHub community articles  ](https://github.com/readme)\nRepositories\n    * [ Topics ](https://github.com/topics)\n    * [ Trending ](https://github.com/trending)\n    * [ Collections ](https://github.com/collections)\n  * Enterprise \n    * [ Enterprise platform  AI-powered developer platform  ](https://github.com/enterprise)\nAvailable add-ons\n    * [ GitHub Advanced Security  Enterprise-grade security features  ](https://github.com/security/advanced-security)\n    * [ Copilot for business  Enterprise-grade AI features  ](https://github.com/features/copilot/copilot-business)\n    * [ Premium Support  Enterprise-grade 24/7 support  ](https://github.com/premium-support)\n  * [Pricing](https://github.com/pricing)\n\n\nSearch or jump to...\n# Search code, repositories, users, issues, pull requests...\nSearch \nClear\n[Search syntax tips](https://docs.github.com/search-github/github-code-search/understanding-github-code-search-syntax)\n#  Provide feedback \nWe read every piece of feedback, and take your input very seriously.\nInclude my email address so I can be contacted\nCancel  Submit feedback \n#  Saved searches \n## Use saved searches to filter your results more quickly\nName\nQuery\nTo see all available qualifiers, see our [documentation](https://docs.github.com/search-github/github-code-search/understanding-github-code-search-syntax). \nCancel  Create saved search \n[ Sign in ](https://github.com/login?return_to=https%3A%2F%2Fgithub.com%2Fconfident-ai%2Fdeepeval)\n[ Sign up ](https://github.com/signup?ref_cta=Sign+up&ref_loc=header+logged+out&ref_page=%2F%3Cuser-name%3E%2F%3Crepo-name%3E&source=header-repo&source_repo=confident-ai%2Fdeepeval)\nAppearance settings\nResetting focus\nYou signed in with another tab or window. [Reload](https://github.com/confident-ai/deepeval) to refresh your session. You signed out in another tab or window. [Reload](https://github.com/confident-ai/deepeval) to refresh your session. You switched accounts on another tab or window. [Reload](https://github.com/confident-ai/deepeval) to refresh your session. Dismiss alert\n{{ message }}\n[ confident-ai ](https://github.com/confident-ai) / **[deepeval](https://github.com/confident-ai/deepeval) ** Public\n  * [ Notifications ](https://github.com/login?return_to=%2Fconfident-ai%2Fdeepeval) You must be signed in to change notification settings\n  * [ Fork 623 ](https://github.com/login?return_to=%2Fconfident-ai%2Fdeepeval)\n  * [ Star  6.8k ](https://github.com/login?return_to=%2Fconfident-ai%2Fdeepeval)\n\n\nThe LLM Evaluation Framework \n[deepeval.com](https://deepeval.com \"https://deepeval.com\")\n### License\n[ Apache-2.0 license ](https://github.com/confident-ai/deepeval/blob/main/LICENSE.md)\n[ 6.8k stars ](https://github.com/confident-ai/deepeval/stargazers) [ 623 forks ](https://github.com/confident-ai/deepeval/forks) [ Branches ](https://github.com/confident-ai/deepeval/branches) [ Tags ](https://github.com/confident-ai/deepeval/tags) [ Activity ](https://github.com/confident-ai/deepeval/activity)\n[ Star  ](https://github.com/login?return_to=%2Fconfident-ai%2Fdeepeval)\n[ Notifications ](https://github.com/login?return_to=%2Fconfident-ai%2Fdeepeval) You must be signed in to change notification settings\n  * [ Code ](https://github.com/confident-ai/deepeval)\n  * [ Issues 157 ](https://github.com/confident-ai/deepeval/issues)\n  * [ Pull requests 15 ](https://github.com/confident-ai/deepeval/pulls)\n  * [ Discussions ](https://github.com/confident-ai/deepeval/discussions)\n  * [ Actions ](https://github.com/confident-ai/deepeval/actions)\n  * [ Projects 0 ](https://github.com/confident-ai/deepeval/projects)\n  * [ Security ](https://github.com/confident-ai/deepeval/security)\n[ ](https://github.com/confident-ai/deepeval/security)\n[ ](https://github.com/confident-ai/deepeval/security)\n[ ](https://github.com/confident-ai/deepeval/security)\n### [ Uh oh!  ](https://github.com/confident-ai/deepeval/security)\n[There was an error while loading. ](https://github.com/confident-ai/deepeval/security)[Please reload this page](https://github.com/confident-ai/deepeval).\n  * [ Insights ](https://github.com/confident-ai/deepeval/pulse)\n\n\nAdditional navigation options\n  * [ Code  ](https://github.com/confident-ai/deepeval)\n  * [ Issues  ](https://github.com/confident-ai/deepeval/issues)\n  * [ Pull requests  ](https://github.com/confident-ai/deepeval/pulls)\n  * [ Discussions  ](https://github.com/confident-ai/deepeval/discussions)\n  * [ Actions  ](https://github.com/confident-ai/deepeval/actions)\n  * [ Projects  ](https://github.com/confident-ai/deepeval/projects)\n  * [ Security  ](https://github.com/confident-ai/deepeval/security)\n  * [ Insights  ](https://github.com/confident-ai/deepeval/pulse)\n\n\n# confident-ai/deepeval\nmain\n[**195** Branches](https://github.com/confident-ai/deepeval/branches)[**247** Tags](https://github.com/confident-ai/deepeval/tags)\n[](https://github.com/confident-ai/deepeval/branches)[](https://github.com/confident-ai/deepeval/tags)\nGo to file\nCode\n## Folders and files\nName| Name| Last commit message| Last commit date  \n---|---|---|---  \n## Latest commit\n[![penguine-ip](https://avatars.githubusercontent.com/u/143328635?v=4&size=40)](https://github.com/penguine-ip)[penguine-ip](https://github.com/confident-ai/deepeval/commits?author=penguine-ip)[Merge pull request](https://github.com/confident-ai/deepeval/commit/028337d0cf616b64b9533d2f172a4ec4ee04cae3) [#1636](https://github.com/confident-ai/deepeval/pull/1636) [from A-Vamshi/fixedFaithfulnessTemplate](https://github.com/confident-ai/deepeval/commit/028337d0cf616b64b9533d2f172a4ec4ee04cae3)May 29, 2025[028337d](https://github.com/confident-ai/deepeval/commit/028337d0cf616b64b9533d2f172a4ec4ee04cae3) Â· May 29, 2025\n## History\n[5,083 Commits](https://github.com/confident-ai/deepeval/commits/main/)[](https://github.com/confident-ai/deepeval/commits/main/)  \n[.github](https://github.com/confident-ai/deepeval/tree/main/.github \".github\")| [.github](https://github.com/confident-ai/deepeval/tree/main/.github \".github\")| [Updated test](https://github.com/confident-ai/deepeval/commit/0acec03be7cb15958e0105949340bfe395cc54a6 \"Updated test\")| Aug 23, 2024  \n[assets](https://github.com/confident-ai/deepeval/tree/main/assets \"assets\")| [assets](https://github.com/confident-ai/deepeval/tree/main/assets \"assets\")| [updated docs](https://github.com/confident-ai/deepeval/commit/001e6606261d4aa0247902b51a41b8552cfbcaac \"updated docs\")| May 7, 2025  \n[deepeval](https://github.com/confident-ai/deepeval/tree/main/deepeval \"deepeval\")| [deepeval](https://github.com/confident-ai/deepeval/tree/main/deepeval \"deepeval\")| [Removed the grammatical errors and threatening language.](https://github.com/confident-ai/deepeval/commit/349baa5bc0e7358b5d5901c5fa4d6ba69e122406 \"Removed the grammatical errors and threatening language.\")| May 29, 2025  \n[docs](https://github.com/confident-ai/deepeval/tree/main/docs \"docs\")| [docs](https://github.com/confident-ai/deepeval/tree/main/docs \"docs\")| [docs update](https://github.com/confident-ai/deepeval/commit/3a5535672524bad27fc7431375cc70e8338300d9 \"docs update\")| May 28, 2025  \n[examples](https://github.com/confident-ai/deepeval/tree/main/examples \"examples\")| [examples](https://github.com/confident-ai/deepeval/tree/main/examples \"examples\")| [many more misspelling typos](https://github.com/confident-ai/deepeval/commit/e26b5e17873be6da598da1299f274bde84892025 \"many more misspelling typos\")| May 2, 2025  \n[tests](https://github.com/confident-ai/deepeval/tree/main/tests \"tests\")| [tests](https://github.com/confident-ai/deepeval/tree/main/tests \"tests\")| [Merge branch 'main' into telemetry/posthog-updates](https://github.com/confident-ai/deepeval/commit/eb2623463574200bb0952c5cab4d417ed9e0f1f1 \"Merge branch 'main' into telemetry/posthog-updates\")| May 18, 2025  \n[tracing_tests](https://github.com/confident-ai/deepeval/tree/main/tracing_tests \"tracing_tests\")| [tracing_tests](https://github.com/confident-ai/deepeval/tree/main/tracing_tests \"tracing_tests\")| [reformat](https://github.com/confident-ai/deepeval/commit/67b5e675f604ddf09a9b810a0b06d0a0b93a6f91 \"reformat\")| May 24, 2025  \n[.gitignore](https://github.com/confident-ai/deepeval/blob/main/.gitignore \".gitignore\")| [.gitignore](https://github.com/confident-ai/deepeval/blob/main/.gitignore \".gitignore\")| [chore: Fix typo in telemetry](https://github.com/confident-ai/deepeval/commit/bf69bcb62bb3223de4d1b478bf466c087b2637d2 \"chore: Fix typo in telemetry\")| Jan 31, 2025  \n[CITATION.cff](https://github.com/confident-ai/deepeval/blob/main/CITATION.cff \"CITATION.cff\")| [CITATION.cff](https://github.com/confident-ai/deepeval/blob/main/CITATION.cff \"CITATION.cff\")| [new release](https://github.com/confident-ai/deepeval/commit/e678f24db32d0e82307e0a4cad80e902d35e7f31 \"new release\")| May 28, 2025  \n[CONTRIBUTING.md](https://github.com/confident-ai/deepeval/blob/main/CONTRIBUTING.md \"CONTRIBUTING.md\")| [CONTRIBUTING.md](https://github.com/confident-ai/deepeval/blob/main/CONTRIBUTING.md \"CONTRIBUTING.md\")| [Update CONTRIBUTING.md](https://github.com/confident-ai/deepeval/commit/c3c942736e504de07608348950692d29eea2a5ff \"Update CONTRIBUTING.md\")| Oct 5, 2024  \n[LICENSE.md](https://github.com/confident-ai/deepeval/blob/main/LICENSE.md \"LICENSE.md\")| [LICENSE.md](https://github.com/confident-ai/deepeval/blob/main/LICENSE.md \"LICENSE.md\")| [Update LICENSE.md](https://github.com/confident-ai/deepeval/commit/d249da8e6249423464926e8db25d2b99df75b602 \"Update LICENSE.md\")| Oct 22, 2024  \n[MANIFEST.in](https://github.com/confident-ai/deepeval/blob/main/MANIFEST.in \"MANIFEST.in\")| [MANIFEST.in](https://github.com/confident-ai/deepeval/blob/main/MANIFEST.in \"MANIFEST.in\")| [Inlcude txt files](https://github.com/confident-ai/deepeval/commit/bcd1cb9af1dbadd4718a655943723be52931f14c \"Inlcude txt files\")| Mar 16, 2024  \n[README.md](https://github.com/confident-ai/deepeval/blob/main/README.md \"README.md\")| [README.md](https://github.com/confident-ai/deepeval/blob/main/README.md \"README.md\")| [Update README.md](https://github.com/confident-ai/deepeval/commit/04ae875f11edd19872775e90dc13e8bdbcdbd22e \"Update README.md\")| May 24, 2025  \n[a.py](https://github.com/confident-ai/deepeval/blob/main/a.py \"a.py\")| [a.py](https://github.com/confident-ai/deepeval/blob/main/a.py \"a.py\")| [.](https://github.com/confident-ai/deepeval/commit/5c8aea6b2b79e74928bc41855b4719870ac5b29b \".\")| Apr 30, 2025  \n[aa.py](https://github.com/confident-ai/deepeval/blob/main/aa.py \"aa.py\")| [aa.py](https://github.com/confident-ai/deepeval/blob/main/aa.py \"aa.py\")| [Fix tracing](https://github.com/confident-ai/deepeval/commit/6a25f016bd48920fc59a1d7ccddbe2720c6f0946 \"Fix tracing\")| May 12, 2025  \n[poetry.lock](https://github.com/confident-ai/deepeval/blob/main/poetry.lock \"poetry.lock\")| [poetry.lock](https://github.com/confident-ai/deepeval/blob/main/poetry.lock \"poetry.lock\")| [removing twine](https://github.com/confident-ai/deepeval/commit/51106a2660289ab540dc442cb5c1c4bd4772a601 \"removing twine\")| May 5, 2025  \n[pyproject.toml](https://github.com/confident-ai/deepeval/blob/main/pyproject.toml \"pyproject.toml\")| [pyproject.toml](https://github.com/confident-ai/deepeval/blob/main/pyproject.toml \"pyproject.toml\")| [new release](https://github.com/confident-ai/deepeval/commit/e678f24db32d0e82307e0a4cad80e902d35e7f31 \"new release\")| May 28, 2025  \n[test_openai_patch.py](https://github.com/confident-ai/deepeval/blob/main/test_openai_patch.py \"test_openai_patch.py\")| [test_openai_patch.py](https://github.com/confident-ai/deepeval/blob/main/test_openai_patch.py \"test_openai_patch.py\")| [Rubric GEval](https://github.com/confident-ai/deepeval/commit/3f83b1a07b02243a76d099c3c495936d93dfb9bd \"Rubric GEval\")| May 15, 2025  \n[test_otel_exporter.py](https://github.com/confident-ai/deepeval/blob/main/test_otel_exporter.py \"test_otel_exporter.py\")| [test_otel_exporter.py](https://github.com/confident-ai/deepeval/blob/main/test_otel_exporter.py \"test_otel_exporter.py\")| [reformta](https://github.com/confident-ai/deepeval/commit/26fe7d80a5aa203b7c0716102580ca4de2b1d35a \"reformta\")| May 22, 2025  \nView all files  \n## Repository files navigation\n  * [README](https://github.com/confident-ai/deepeval)\n  * [Apache-2.0 license](https://github.com/confident-ai/deepeval)\n\n\n[![DeepEval Logo](https://github.com/confident-ai/deepeval/raw/main/docs/static/img/deepeval.png)](https://github.com/confident-ai/deepeval/blob/main/docs/static/img/deepeval.png)\n# The LLM Evaluation Framework\n[](https://github.com/confident-ai/deepeval#the-llm-evaluation-framework)\n[ ![discord-invite](https://camo.githubusercontent.com/7f735ffad46bc6bf7af809772427a5c2ffb209950fc97e2bbc3a67a1451d866d/68747470733a2f2f646362616467652e76657263656c2e6170702f6170692f7365727665722f335345797670677532663f7374796c653d666c6174) ](https://discord.gg/3SEyvpgu2f)\n[Documentation](https://deepeval.com/docs/getting-started?utm_source=GitHub) | [Metrics and Features](https://github.com/confident-ai/deepeval#-metrics-and-features) | [Getting Started](https://github.com/confident-ai/deepeval#-quickstart) | [Integrations](https://github.com/confident-ai/deepeval#-integrations) | [DeepEval Platform](https://confident-ai.com?utm_source=GitHub)\n[](https://github.com/confident-ai/deepeval#------------documentation---------metrics-and-features---------getting-started---------integrations---------deepeval-platform----)\n[ ![GitHub release](https://camo.githubusercontent.com/9c138e3fbe391567af9b8bd7f705cd63fc5e2ccdb38d7a34f978cfd225efb722/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f72656c656173652f636f6e666964656e742d61692f646565706576616c2e7376673f636f6c6f723d76696f6c6574) ](https://github.com/confident-ai/deepeval/releases) [ ![Try Quickstart in Colab](https://camo.githubusercontent.com/96889048f8a9014fdeba2a891f97150c6aac6e723f5190236b10215a97ed41f3/68747470733a2f2f636f6c61622e72657365617263682e676f6f676c652e636f6d2f6173736574732f636f6c61622d62616467652e737667) ](https://colab.research.google.com/drive/1PPxYEBa6eu__LquGoFFJZkhYgWVYE6kh?usp=sharing) [ ![License](https://camo.githubusercontent.com/27470c836c4530fafb1823b39732d4607805a572861be9dab56413c7faa3ab85/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f6c6963656e73652f636f6e666964656e742d61692f646565706576616c2e7376673f636f6c6f723d79656c6c6f77) ](https://github.com/confident-ai/deepeval/blob/master/LICENSE.md)\n**DeepEval** is a simple-to-use, open-source LLM evaluation framework, for evaluating and testing large-language model systems. It is similar to Pytest but specialized for unit testing LLM outputs. DeepEval incorporates the latest research to evaluate LLM outputs based on metrics such as G-Eval, hallucination, answer relevancy, RAGAS, etc., which uses LLMs and various other NLP models that runs **locally on your machine** for evaluation.\nWhether your LLM applications are RAG pipelines, chatbots, AI agents, implemented via LangChain or LlamaIndex, DeepEval has you covered. With it, you can easily determine the optimal models, prompts, and architecture to improve your RAG pipeline, agentic workflows, prevent prompt drifting, or even transition from OpenAI to hosting your own Deepseek R1 with confidence.\nImportant\nNeed a place for your DeepEval testing data to live ð¡â¤ï¸? [Sign up to the DeepEval platform](https://confident-ai.com?utm_source=GitHub) to compare iterations of your LLM app, generate & share testing reports, and more.\n[![Demo GIF](https://github.com/confident-ai/deepeval/raw/main/assets/demo.gif)](https://github.com/confident-ai/deepeval/blob/main/assets/demo.gif) [ ![Demo GIF](https://github.com/confident-ai/deepeval/raw/main/assets/demo.gif) ](https://github.com/confident-ai/deepeval/blob/main/assets/demo.gif) [ ](https://github.com/confident-ai/deepeval/blob/main/assets/demo.gif)\n> Want to talk LLM evaluation, need help picking metrics, or just to say hi? [Come join our discord.](https://discord.com/invite/3SEyvpgu2f)\n# ð¥ Metrics and Features\n[](https://github.com/confident-ai/deepeval#-metrics-and-features)\n> ð¥³ You can now share DeepEval's test results on the cloud directly on [Confident AI](https://confident-ai.com?utm_source=GitHub)'s infrastructure\n  * Supports both end-to-end and component-level LLM evaluation.\n  * Large variety of ready-to-use LLM evaluation metrics (all with explanations) powered by **ANY** LLM of your choice, statistical methods, or NLP models that runs **locally on your machine** : \n    * G-Eval\n    * DAG ([deep acyclic graph](https://deepeval.com/docs/metrics-dag))\n    * **RAG metrics:**\n      * Answer Relevancy\n      * Faithfulness\n      * Contextual Recall\n      * Contextual Precision\n      * Contextual Relevancy\n      * RAGAS\n    * **Agentic metrics:**\n      * Task Completion\n      * Tool Correctness\n    * **Others:**\n      * Hallucination\n      * Summarization\n      * Bias\n      * Toxicity\n    * **Conversational metrics:**\n      * Knowledge Retention\n      * Conversation Completeness\n      * Conversation Relevancy\n      * Role Adherence\n    * etc.\n  * Build your own custom metrics that are automatically integrated with DeepEval's ecosystem.\n  * Generate synthetic datasets for evaluation.\n  * Integrates seamlessly with **ANY** CI/CD environment.\n  * [Red team your LLM application](https://deepeval.com/docs/red-teaming-introduction) for 40+ safety vulnerabilities in a few lines of code, including: \n    * Toxicity\n    * Bias\n    * SQL Injection\n    * etc., using advanced 10+ attack enhancement strategies such as prompt injections.\n  * Easily benchmark **ANY** LLM on popular LLM benchmarks in [under 10 lines of code.](https://deepeval.com/docs/benchmarks-introduction?utm_source=GitHub), which includes: \n    * MMLU\n    * HellaSwag\n    * DROP\n    * BIG-Bench Hard\n    * TruthfulQA\n    * HumanEval\n    * GSM8K\n  * [100% integrated with Confident AI](https://confident-ai.com?utm_source=GitHub) for the full evaluation lifecycle: \n    * Curate/annotate evaluation datasets on the cloud\n    * Benchmark LLM app using dataset, and compare with previous iterations to experiment which models/prompts works best\n    * Fine-tune metrics for custom results\n    * Debug evaluation results via LLM traces\n    * Monitor & evaluate LLM responses in product to improve datasets with real-world data\n    * Repeat until perfection\n\n\nNote\nConfident AI is the DeepEval platform. Create an account [here.](https://app.confident-ai.com?utm_source=GitHub)\n# ð Integrations\n[](https://github.com/confident-ai/deepeval#-integrations)\n  * ð¦ LlamaIndex, to [**unit test RAG applications in CI/CD**](https://www.deepeval.com/integrations/frameworks/llamaindex?utm_source=GitHub)\n  * ð¤ Hugging Face, to [**enable real-time evaluations during LLM fine-tuning**](https://www.deepeval.com/integrations/frameworks/huggingface?utm_source=GitHub)\n\n\n# ð QuickStart\n[](https://github.com/confident-ai/deepeval#-quickstart)\nLet's pretend your LLM application is a RAG based customer support chatbot; here's how DeepEval can help test what you've built.\n## Installation\n[](https://github.com/confident-ai/deepeval#installation)\n```\npip install -U deepeval\n\n```\n\n## Create an account (highly recommended)\n[](https://github.com/confident-ai/deepeval#create-an-account-highly-recommended)\nUsing the `deepeval` platform will allow you to generate sharable testing reports on the cloud. It is free, takes no additional code to setup, and we highly recommend giving it a try.\nTo login, run:\n```\ndeepeval login\n\n```\n\nFollow the instructions in the CLI to create an account, copy your API key, and paste it into the CLI. All test cases will automatically be logged (find more information on data privacy [here](https://deepeval.com/docs/data-privacy?utm_source=GitHub)).\n## Writing your first test case\n[](https://github.com/confident-ai/deepeval#writing-your-first-test-case)\nCreate a test file:\n```\ntouch test_chatbot.py\n```\n\nOpen `test_chatbot.py` and write your first test case to run an **end-to-end** evaluation using DeepEval, which treats your LLM app as a black-box:\n```\nimport pytest\nfrom deepeval import assert_test\nfrom deepeval.metrics import GEval\nfrom deepeval.test_case import LLMTestCase, LLMTestCaseParams\ndef test_case():\n  correctness_metric = GEval(\n    name=\"Correctness\",\n    criteria=\"Determine if the 'actual output' is correct based on the 'expected output'.\",\n    evaluation_params=[LLMTestCaseParams.ACTUAL_OUTPUT, LLMTestCaseParams.EXPECTED_OUTPUT],\n    threshold=0.5\n  )\n  test_case = LLMTestCase(\n    input=\"What if these shoes don't fit?\",\n    # Replace this with the actual output from your LLM application\n    actual_output=\"You have 30 days to get a full refund at no extra cost.\",\n    expected_output=\"We offer a 30-day full refund at no extra costs.\",\n    retrieval_context=[\"All customers are eligible for a 30 day full refund at no extra costs.\"]\n  )\n  assert_test(test_case, [correctness_metric])\n```\n\nSet your `OPENAI_API_KEY` as an environment variable (you can also evaluate using your own custom model, for more details visit [this part of our docs](https://deepeval.com/docs/metrics-introduction#using-a-custom-llm?utm_source=GitHub)):\n```\nexport OPENAI_API_KEY=\"...\"\n\n```\n\nAnd finally, run `test_chatbot.py` in the CLI:\n```\ndeepeval test run test_chatbot.py\n\n```\n\n**Congratulations! Your test case should have passed â** Let's breakdown what happened.\n  * The variable `input` mimics a user input, and `actual_output` is a placeholder for what your application's supposed to output based on this input.\n  * The variable `expected_output` represents the ideal answer for a given `input`, and [`GEval`](https://deepeval.com/docs/metrics-llm-evals) is a research-backed metric provided by `deepeval` for you to evaluate your LLM output's on any custom custom with human-like accuracy.\n  * In this example, the metric `criteria` is correctness of the `actual_output` based on the provided `expected_output`.\n  * All metric scores range from 0 - 1, which the `threshold=0.5` threshold ultimately determines if your test have passed or not.\n\n\n[Read our documentation](https://deepeval.com/docs/getting-started?utm_source=GitHub) for more information on more options to run end-to-end evaluation, how to use additional metrics, create your own custom metrics, and tutorials on how to integrate with other tools like LangChain and LlamaIndex.\n## Evaluating Nested Components\n[](https://github.com/confident-ai/deepeval#evaluating-nested-components)\nIf you wish to evaluate individual components within your LLM app, you need to run **component-level** evals - a powerful way to evaluate any component within an LLM system.\nSimply trace \"components\" such as LLM calls, retrievers, tool calls, and agents within your LLM application using the `@observe` decorator to apply metrics on a component-level. Tracing with `deepeval` is non-instrusive (learn more [here](https://deepeval.com/docs/evaluation-llm-tracing#dont-be-worried-about-tracing)) and helps you avoid rewriting your codebase just for evals:\n```\nfrom deepeval.tracing import observe, update_current_span\nfrom deepeval.test_case import LLMTestCase\nfrom deepeval.dataset import Golden\nfrom deepeval.metrics import GEval\nfrom deepeval import evaluate\ncorrectness = GEval(name=\"Correctness\", criteria=\"Determine if the 'actual output' is correct based on the 'expected output'.\", evaluation_params=[LLMTestCaseParams.ACTUAL_OUTPUT, LLMTestCaseParams.EXPECTED_OUTPUT])\n@observe(metrics=[correctness])\ndef inner_component():\n  # Component can be anything from an LLM call, retrieval, agent, tool use, etc.\n  update_current_span(test_case=LLMTestCase(input=\"...\", actual_output=\"...\"))\n  return\n@observe\ndef llm_app(input: str):\n  inner_component()\n  return\nevaluate(observed_callback=llm_app, goldens=[Golden(input=\"Hi!\")])\n```\n\nYou can learn everything about component-level evaluations [here.](https://www.deepeval.com/docs/evaluation-component-level-llm-evals)\n## Evaluating Without Pytest Integration\n[](https://github.com/confident-ai/deepeval#evaluating-without-pytest-integration)\nAlternatively, you can evaluate without Pytest, which is more suited for a notebook environment.\n```\nfrom deepeval import evaluate\nfrom deepeval.metrics import AnswerRelevancyMetric\nfrom deepeval.test_case import LLMTestCase\nanswer_relevancy_metric = AnswerRelevancyMetric(threshold=0.7)\ntest_case = LLMTestCase(\n  input=\"What if these shoes don't fit?\",\n  # Replace this with the actual output from your LLM application\n  actual_output=\"We offer a 30-day full refund at no extra costs.\",\n  retrieval_context=[\"All customers are eligible for a 30 day full refund at no extra costs.\"]\n)\nevaluate([test_case], [answer_relevancy_metric])\n```\n\n## Using Standalone Metrics\n[](https://github.com/confident-ai/deepeval#using-standalone-metrics)\nDeepEval is extremely modular, making it easy for anyone to use any of our metrics. Continuing from the previous example:\n```\nfrom deepeval.metrics import AnswerRelevancyMetric\nfrom deepeval.test_case import LLMTestCase\nanswer_relevancy_metric = AnswerRelevancyMetric(threshold=0.7)\ntest_case = LLMTestCase(\n  input=\"What if these shoes don't fit?\",\n  # Replace this with the actual output from your LLM application\n  actual_output=\"We offer a 30-day full refund at no extra costs.\",\n  retrieval_context=[\"All customers are eligible for a 30 day full refund at no extra costs.\"]\n)\nanswer_relevancy_metric.measure(test_case)\nprint(answer_relevancy_metric.score)\n# All metrics also offer an explanation\nprint(answer_relevancy_metric.reason)\n```\n\nNote that some metrics are for RAG pipelines, while others are for fine-tuning. Make sure to use our docs to pick the right one for your use case.\n## Evaluating a Dataset / Test Cases in Bulk\n[](https://github.com/confident-ai/deepeval#evaluating-a-dataset--test-cases-in-bulk)\nIn DeepEval, a dataset is simply a collection of test cases. Here is how you can evaluate these in bulk:\n```\nimport pytest\nfrom deepeval import assert_test\nfrom deepeval.metrics import HallucinationMetric, AnswerRelevancyMetric\nfrom deepeval.test_case import LLMTestCase\nfrom deepeval.dataset import EvaluationDataset\nfirst_test_case = LLMTestCase(input=\"...\", actual_output=\"...\", context=[\"...\"])\nsecond_test_case = LLMTestCase(input=\"...\", actual_output=\"...\", context=[\"...\"])\ndataset = EvaluationDataset(test_cases=[first_test_case, second_test_case])\n@pytest.mark.parametrize(\n  \"test_case\",\n  dataset,\n)\ndef test_customer_chatbot(test_case: LLMTestCase):\n  hallucination_metric = HallucinationMetric(threshold=0.3)\n  answer_relevancy_metric = AnswerRelevancyMetric(threshold=0.5)\n  assert_test(test_case, [hallucination_metric, answer_relevancy_metric])\n```\n\n```\n# Run this in the CLI, you can also add an optional -n flag to run tests in parallel\ndeepeval test run test_<filename>.py -n 4\n```\n\nAlternatively, although we recommend using `deepeval test run`, you can evaluate a dataset/test cases without using our Pytest integration:\n```\nfrom deepeval import evaluate\n...\nevaluate(dataset, [answer_relevancy_metric])\n# or\ndataset.evaluate([answer_relevancy_metric])\n```\n\n# LLM Evaluation With Confident AI\n[](https://github.com/confident-ai/deepeval#llm-evaluation-with-confident-ai)\nThe correct LLM evaluation lifecycle is only achievable with [the DeepEval platform](https://confident-ai.com?utm_source=Github). It allows you to:\n  1. Curate/annotate evaluation datasets on the cloud\n  2. Benchmark LLM app using dataset, and compare with previous iterations to experiment which models/prompts works best\n  3. Fine-tune metrics for custom results\n  4. Debug evaluation results via LLM traces\n  5. Monitor & evaluate LLM responses in product to improve datasets with real-world data\n  6. Repeat until perfection\n\n\nEverything on Confident AI, including how to use Confident is available [here](https://documentation.confident-ai.com?utm_source=GitHub).\nTo begin, login from the CLI:\n```\ndeepeval login\n```\n\nFollow the instructions to log in, create your account, and paste your API key into the CLI.\nNow, run your test file again:\n```\ndeepeval test run test_chatbot.py\n```\n\nYou should see a link displayed in the CLI once the test has finished running. Paste it into your browser to view the results!\n[![Demo GIF](https://github.com/confident-ai/deepeval/raw/main/assets/demo.gif)](https://github.com/confident-ai/deepeval/blob/main/assets/demo.gif) [ ![Demo GIF](https://github.com/confident-ai/deepeval/raw/main/assets/demo.gif) ](https://github.com/confident-ai/deepeval/blob/main/assets/demo.gif) [ ](https://github.com/confident-ai/deepeval/blob/main/assets/demo.gif)\n# Contributing\n[](https://github.com/confident-ai/deepeval#contributing)\nPlease read [CONTRIBUTING.md](https://github.com/confident-ai/deepeval/blob/main/CONTRIBUTING.md) for details on our code of conduct, and the process for submitting pull requests to us.\n# Roadmap\n[](https://github.com/confident-ai/deepeval#roadmap)\nFeatures:\n  * Integration with Confident AI\n  * Implement G-Eval\n  * Implement RAG metrics\n  * Implement Conversational metrics\n  * Evaluation Dataset Creation\n  * Red-Teaming\n  * DAG custom metrics\n  * Guardrails\n\n\n# Authors\n[](https://github.com/confident-ai/deepeval#authors)\nBuilt by the founders of Confident AI. Contact jeffreyip@confident-ai.com for all enquiries.\n# License\n[](https://github.com/confident-ai/deepeval#license)\nDeepEval is licensed under Apache 2.0 - see the [LICENSE.md](https://github.com/confident-ai/deepeval/blob/main/LICENSE.md) file for details.\n## About\nThe LLM Evaluation Framework \n[deepeval.com](https://deepeval.com \"https://deepeval.com\")\n### Topics\n[ evaluation-metrics ](https://github.com/topics/evaluation-metrics \"Topic: evaluation-metrics\") [ evaluation-framework ](https://github.com/topics/evaluation-framework \"Topic: evaluation-framework\") [ llm-evaluation ](https://github.com/topics/llm-evaluation \"Topic: llm-evaluation\") [ llm-evaluation-framework ](https://github.com/topics/llm-evaluation-framework \"Topic: llm-evaluation-framework\") [ llm-evaluation-metrics ](https://github.com/topics/llm-evaluation-metrics \"Topic: llm-evaluation-metrics\")\n### Resources\n[ Readme ](https://github.com/confident-ai/deepeval#readme-ov-file)\n### License\n[ Apache-2.0 license ](https://github.com/confident-ai/deepeval#Apache-2.0-1-ov-file)\n### Citation\nCite this repository \nLoading\nSomething went wrong. \n###  Uh oh! \nThere was an error while loading. [Please reload this page](https://github.com/confident-ai/deepeval).\n[ Activity](https://github.com/confident-ai/deepeval/activity)\n[ Custom properties](https://github.com/confident-ai/deepeval/custom-properties)\n### Stars\n[ **6.8k** stars](https://github.com/confident-ai/deepeval/stargazers)\n### Watchers\n[ **31** watching](https://github.com/confident-ai/deepeval/watchers)\n### Forks\n[ **623** forks](https://github.com/confident-ai/deepeval/forks)\n[ Report repository ](https://github.com/contact/report-content?content_url=https%3A%2F%2Fgithub.com%2Fconfident-ai%2Fdeepeval&report=confident-ai+%28user%29)\n##  [Releases 46](https://github.com/confident-ai/deepeval/releases)\n[ LLM Evals - v3.0 Latest  May 27, 2025 ](https://github.com/confident-ai/deepeval/releases/tag/v3.0)\n[+ 45 releases](https://github.com/confident-ai/deepeval/releases)\n##  [Packages 0](https://github.com/orgs/confident-ai/packages?repo_name=deepeval)\nNo packages published \n##  [Used by 803](https://github.com/confident-ai/deepeval/network/dependents)\n[\n  * ![@Gyana1308](https://avatars.githubusercontent.com/u/186584960?s=64&v=4)\n  * ![@DakineMI](https://avatars.githubusercontent.com/u/25511074?s=64&v=4)\n  * ![@DakineMI](https://avatars.githubusercontent.com/u/25511074?s=64&v=4)\n  * ![@mjm3853](https://avatars.githubusercontent.com/u/7984928?s=64&v=4)\n  * ![@git-aethrix](https://avatars.githubusercontent.com/u/16635199?s=64&v=4)\n  * ![@hbasafa](https://avatars.githubusercontent.com/u/30258093?s=64&v=4)\n  * ![@TunsTudor-Mircea](https://avatars.githubusercontent.com/u/162615897?s=64&v=4)\n  * ![@gustavolgcr](https://avatars.githubusercontent.com/u/1895558?s=64&v=4)\n\n+ 795  ](https://github.com/confident-ai/deepeval/network/dependents)\n##  [Contributors 153](https://github.com/confident-ai/deepeval/graphs/contributors)\n  * [ ![@penguine-ip](https://avatars.githubusercontent.com/u/143328635?s=64&v=4) ](https://github.com/penguine-ip)\n  * [ ![@jwongster2](https://avatars.githubusercontent.com/u/108557828?s=64&v=4) ](https://github.com/jwongster2)\n  * [ ![@kritinv](https://avatars.githubusercontent.com/u/73642562?s=64&v=4) ](https://github.com/kritinv)\n  * [ ![@Anindyadeep](https://avatars.githubusercontent.com/u/58508471?s=64&v=4) ](https://github.com/Anindyadeep)\n  * [ ![@Vasilije1990](https://avatars.githubusercontent.com/u/8619304?s=64&v=4) ](https://github.com/Vasilije1990)\n  * [ ![@Pratyush-exe](https://avatars.githubusercontent.com/u/78687109?s=64&v=4) ](https://github.com/Pratyush-exe)\n  * [ ![@agokrani](https://avatars.githubusercontent.com/u/30440108?s=64&v=4) ](https://github.com/agokrani)\n  * [ ![@spike-spiegel-21](https://avatars.githubusercontent.com/u/83648453?s=64&v=4) ](https://github.com/spike-spiegel-21)\n  * [ ![@fetz236](https://avatars.githubusercontent.com/u/58368484?s=64&v=4) ](https://github.com/fetz236)\n  * [ ![@Peilun-Li](https://avatars.githubusercontent.com/u/11920339?s=64&v=4) ](https://github.com/Peilun-Li)\n  * [ ![@luarss](https://avatars.githubusercontent.com/u/39641663?s=64&v=4) ](https://github.com/luarss)\n  * [ ![@lesar64](https://avatars.githubusercontent.com/u/54540187?s=64&v=4) ](https://github.com/lesar64)\n  * [ ![@fschuh](https://avatars.githubusercontent.com/u/12468976?s=64&v=4) ](https://github.com/fschuh)\n  * [ ![@john-lemmon-lime](https://avatars.githubusercontent.com/u/6528428?s=64&v=4) ](https://github.com/john-lemmon-lime)\n\n\n[+ 139 contributors](https://github.com/confident-ai/deepeval/graphs/contributors)\n## Languages\n  * [ Python 100.0% ](https://github.com/confident-ai/deepeval/search?l=python)\n\n\n## Footer\n[ ](https://github.com) Â© 2025 GitHub, Inc. \n### Footer navigation\n  * [Terms](https://docs.github.com/site-policy/github-terms/github-terms-of-service)\n  * [Privacy](https://docs.github.com/site-policy/privacy-policies/github-privacy-statement)\n  * [Security](https://github.com/security)\n  * [Status](https://www.githubstatus.com/)\n  * [Docs](https://docs.github.com/)\n  * [Contact](https://support.github.com?tags=dotcom-footer)\n  * Manage cookies \n  * Do not share my personal information \n\n\nYou canât perform that action at this time. \n"
    },
    {
      "link": "https://github.com/Azure/The-LLM-Latency-Guidebook-Optimizing-Response-Times-for-GenAI-Applications",
      "raw_content": "[Skip to content](https://github.com/Azure/The-LLM-Latency-Guidebook-Optimizing-Response-Times-for-GenAI-Applications#start-of-content)\n## Navigation Menu\nToggle navigation\n[ ](https://github.com/)\n[ Sign in ](https://github.com/login?return_to=https%3A%2F%2Fgithub.com%2FAzure%2FThe-LLM-Latency-Guidebook-Optimizing-Response-Times-for-GenAI-Applications)\nAppearance settings\n  * Product \n    * [ GitHub Copilot  Write better code with AI  ](https://github.com/features/copilot)\n    * [ GitHub Models  New  Manage and compare prompts  ](https://github.com/features/models)\n    * [ GitHub Advanced Security  Find and fix vulnerabilities  ](https://github.com/security/advanced-security)\n    * [ Actions  Automate any workflow  ](https://github.com/features/actions)\n    * [ Codespaces  Instant dev environments  ](https://github.com/features/codespaces)\n    * [ Issues  Plan and track work  ](https://github.com/features/issues)\n    * [ Code Review  Manage code changes  ](https://github.com/features/code-review)\n    * [ Discussions  Collaborate outside of code  ](https://github.com/features/discussions)\n    * [ Code Search  Find more, search less  ](https://github.com/features/code-search)\nExplore\n    * [ Why GitHub ](https://github.com/why-github)\n    * [ All features ](https://github.com/features)\n    * [ Documentation ](https://docs.github.com)\n    * [ GitHub Skills ](https://skills.github.com)\n    * [ Blog ](https://github.blog)\n  * Solutions \nBy company size\n    * [ Enterprises ](https://github.com/enterprise)\n    * [ Small and medium teams ](https://github.com/team)\n    * [ Startups ](https://github.com/enterprise/startups)\n    * [ Nonprofits ](https://github.com/solutions/industry/nonprofits)\nBy use case\n    * [ DevSecOps ](https://github.com/solutions/use-case/devsecops)\n    * [ DevOps ](https://github.com/solutions/use-case/devops)\n    * [ CI/CD ](https://github.com/solutions/use-case/ci-cd)\n    * [ View all use cases ](https://github.com/solutions/use-case)\nBy industry\n    * [ Healthcare ](https://github.com/solutions/industry/healthcare)\n    * [ Financial services ](https://github.com/solutions/industry/financial-services)\n    * [ Manufacturing ](https://github.com/solutions/industry/manufacturing)\n    * [ Government ](https://github.com/solutions/industry/government)\n    * [ View all industries ](https://github.com/solutions/industry)\n[ View all solutions ](https://github.com/solutions)\n  * Resources \nTopics\n    * [ AI ](https://github.com/resources/articles/ai)\n    * [ DevOps ](https://github.com/resources/articles/devops)\n    * [ Security ](https://github.com/resources/articles/security)\n    * [ Software Development ](https://github.com/resources/articles/software-development)\n    * [ View all ](https://github.com/resources/articles)\nExplore\n    * [ Learning Pathways ](https://resources.github.com/learn/pathways)\n    * [ Events & Webinars ](https://resources.github.com)\n    * [ Ebooks & Whitepapers ](https://github.com/resources/whitepapers)\n    * [ Customer Stories ](https://github.com/customer-stories)\n    * [ Partners ](https://partner.github.com)\n    * [ Executive Insights ](https://github.com/solutions/executive-insights)\n  * Open Source \n    * [ GitHub Sponsors  Fund open source developers  ](https://github.com/sponsors)\n    * [ The ReadME Project  GitHub community articles  ](https://github.com/readme)\nRepositories\n    * [ Topics ](https://github.com/topics)\n    * [ Trending ](https://github.com/trending)\n    * [ Collections ](https://github.com/collections)\n  * Enterprise \n    * [ Enterprise platform  AI-powered developer platform  ](https://github.com/enterprise)\nAvailable add-ons\n    * [ GitHub Advanced Security  Enterprise-grade security features  ](https://github.com/security/advanced-security)\n    * [ Copilot for business  Enterprise-grade AI features  ](https://github.com/features/copilot/copilot-business)\n    * [ Premium Support  Enterprise-grade 24/7 support  ](https://github.com/premium-support)\n  * [Pricing](https://github.com/pricing)\n\n\nSearch or jump to...\n# Search code, repositories, users, issues, pull requests...\nSearch \nClear\n[Search syntax tips](https://docs.github.com/search-github/github-code-search/understanding-github-code-search-syntax)\n#  Provide feedback \nWe read every piece of feedback, and take your input very seriously.\nInclude my email address so I can be contacted\nCancel  Submit feedback \n#  Saved searches \n## Use saved searches to filter your results more quickly\nName\nQuery\nTo see all available qualifiers, see our [documentation](https://docs.github.com/search-github/github-code-search/understanding-github-code-search-syntax). \nCancel  Create saved search \n[ Sign in ](https://github.com/login?return_to=https%3A%2F%2Fgithub.com%2FAzure%2FThe-LLM-Latency-Guidebook-Optimizing-Response-Times-for-GenAI-Applications)\n[ Sign up ](https://github.com/signup?ref_cta=Sign+up&ref_loc=header+logged+out&ref_page=%2F%3Cuser-name%3E%2F%3Crepo-name%3E&source=header-repo&source_repo=Azure%2FThe-LLM-Latency-Guidebook-Optimizing-Response-Times-for-GenAI-Applications)\nAppearance settings\nResetting focus\nYou signed in with another tab or window. [Reload](https://github.com/Azure/The-LLM-Latency-Guidebook-Optimizing-Response-Times-for-GenAI-Applications) to refresh your session. You signed out in another tab or window. [Reload](https://github.com/Azure/The-LLM-Latency-Guidebook-Optimizing-Response-Times-for-GenAI-Applications) to refresh your session. You switched accounts on another tab or window. [Reload](https://github.com/Azure/The-LLM-Latency-Guidebook-Optimizing-Response-Times-for-GenAI-Applications) to refresh your session. Dismiss alert\n{{ message }}\n[ Azure ](https://github.com/Azure) / **[The-LLM-Latency-Guidebook-Optimizing-Response-Times-for-GenAI-Applications](https://github.com/Azure/The-LLM-Latency-Guidebook-Optimizing-Response-Times-for-GenAI-Applications) ** Public\n  * [ Notifications ](https://github.com/login?return_to=%2FAzure%2FThe-LLM-Latency-Guidebook-Optimizing-Response-Times-for-GenAI-Applications) You must be signed in to change notification settings\n  * [ Fork 8 ](https://github.com/login?return_to=%2FAzure%2FThe-LLM-Latency-Guidebook-Optimizing-Response-Times-for-GenAI-Applications)\n  * [ Star  27 ](https://github.com/login?return_to=%2FAzure%2FThe-LLM-Latency-Guidebook-Optimizing-Response-Times-for-GenAI-Applications)\n\n\nThere are many articles that cover the principles of reducing latency optimization for LLMs, however it is often unclear how to actually implement these principles. This repository provides practical techniques for reducing the latency of GenAI applications. \n### License\n[ MIT license ](https://github.com/Azure/The-LLM-Latency-Guidebook-Optimizing-Response-Times-for-GenAI-Applications/blob/main/LICENSE)\n[ 27 stars ](https://github.com/Azure/The-LLM-Latency-Guidebook-Optimizing-Response-Times-for-GenAI-Applications/stargazers) [ 8 forks ](https://github.com/Azure/The-LLM-Latency-Guidebook-Optimizing-Response-Times-for-GenAI-Applications/forks) [ Branches ](https://github.com/Azure/The-LLM-Latency-Guidebook-Optimizing-Response-Times-for-GenAI-Applications/branches) [ Tags ](https://github.com/Azure/The-LLM-Latency-Guidebook-Optimizing-Response-Times-for-GenAI-Applications/tags) [ Activity ](https://github.com/Azure/The-LLM-Latency-Guidebook-Optimizing-Response-Times-for-GenAI-Applications/activity)\n[ Star  ](https://github.com/login?return_to=%2FAzure%2FThe-LLM-Latency-Guidebook-Optimizing-Response-Times-for-GenAI-Applications)\n[ Notifications ](https://github.com/login?return_to=%2FAzure%2FThe-LLM-Latency-Guidebook-Optimizing-Response-Times-for-GenAI-Applications) You must be signed in to change notification settings\n  * [ Code ](https://github.com/Azure/The-LLM-Latency-Guidebook-Optimizing-Response-Times-for-GenAI-Applications)\n  * [ Issues 1 ](https://github.com/Azure/The-LLM-Latency-Guidebook-Optimizing-Response-Times-for-GenAI-Applications/issues)\n  * [ Pull requests 0 ](https://github.com/Azure/The-LLM-Latency-Guidebook-Optimizing-Response-Times-for-GenAI-Applications/pulls)\n  * [ Actions ](https://github.com/Azure/The-LLM-Latency-Guidebook-Optimizing-Response-Times-for-GenAI-Applications/actions)\n  * [ Projects 0 ](https://github.com/Azure/The-LLM-Latency-Guidebook-Optimizing-Response-Times-for-GenAI-Applications/projects)\n  * [ Security ](https://github.com/Azure/The-LLM-Latency-Guidebook-Optimizing-Response-Times-for-GenAI-Applications/security)\n[ ](https://github.com/Azure/The-LLM-Latency-Guidebook-Optimizing-Response-Times-for-GenAI-Applications/security)\n[ ](https://github.com/Azure/The-LLM-Latency-Guidebook-Optimizing-Response-Times-for-GenAI-Applications/security)\n[ ](https://github.com/Azure/The-LLM-Latency-Guidebook-Optimizing-Response-Times-for-GenAI-Applications/security)\n### [ Uh oh!  ](https://github.com/Azure/The-LLM-Latency-Guidebook-Optimizing-Response-Times-for-GenAI-Applications/security)\n[There was an error while loading. ](https://github.com/Azure/The-LLM-Latency-Guidebook-Optimizing-Response-Times-for-GenAI-Applications/security)[Please reload this page](https://github.com/Azure/The-LLM-Latency-Guidebook-Optimizing-Response-Times-for-GenAI-Applications).\n  * [ Insights ](https://github.com/Azure/The-LLM-Latency-Guidebook-Optimizing-Response-Times-for-GenAI-Applications/pulse)\n\n\nAdditional navigation options\n  * [ Code  ](https://github.com/Azure/The-LLM-Latency-Guidebook-Optimizing-Response-Times-for-GenAI-Applications)\n  * [ Issues  ](https://github.com/Azure/The-LLM-Latency-Guidebook-Optimizing-Response-Times-for-GenAI-Applications/issues)\n  * [ Pull requests  ](https://github.com/Azure/The-LLM-Latency-Guidebook-Optimizing-Response-Times-for-GenAI-Applications/pulls)\n  * [ Actions  ](https://github.com/Azure/The-LLM-Latency-Guidebook-Optimizing-Response-Times-for-GenAI-Applications/actions)\n  * [ Projects  ](https://github.com/Azure/The-LLM-Latency-Guidebook-Optimizing-Response-Times-for-GenAI-Applications/projects)\n  * [ Security  ](https://github.com/Azure/The-LLM-Latency-Guidebook-Optimizing-Response-Times-for-GenAI-Applications/security)\n  * [ Insights  ](https://github.com/Azure/The-LLM-Latency-Guidebook-Optimizing-Response-Times-for-GenAI-Applications/pulse)\n\n\n# Azure/The-LLM-Latency-Guidebook-Optimizing-Response-Times-for-GenAI-Applications\nmain\n[Branches](https://github.com/Azure/The-LLM-Latency-Guidebook-Optimizing-Response-Times-for-GenAI-Applications/branches)[Tags](https://github.com/Azure/The-LLM-Latency-Guidebook-Optimizing-Response-Times-for-GenAI-Applications/tags)\n[](https://github.com/Azure/The-LLM-Latency-Guidebook-Optimizing-Response-Times-for-GenAI-Applications/branches)[](https://github.com/Azure/The-LLM-Latency-Guidebook-Optimizing-Response-Times-for-GenAI-Applications/tags)\nGo to file\nCode\n## Folders and files\nName| Name| Last commit message| Last commit date  \n---|---|---|---  \n## Latest commit\n## History\n[6 Commits](https://github.com/Azure/The-LLM-Latency-Guidebook-Optimizing-Response-Times-for-GenAI-Applications/commits/main/)[](https://github.com/Azure/The-LLM-Latency-Guidebook-Optimizing-Response-Times-for-GenAI-Applications/commits/main/)  \n[case-studies](https://github.com/Azure/The-LLM-Latency-Guidebook-Optimizing-Response-Times-for-GenAI-Applications/tree/main/case-studies \"case-studies\")| [case-studies](https://github.com/Azure/The-LLM-Latency-Guidebook-Optimizing-Response-Times-for-GenAI-Applications/tree/main/case-studies \"case-studies\")| |   \n[notebooks-with-techniques](https://github.com/Azure/The-LLM-Latency-Guidebook-Optimizing-Response-Times-for-GenAI-Applications/tree/main/notebooks-with-techniques \"notebooks-with-techniques\")| [notebooks-with-techniques](https://github.com/Azure/The-LLM-Latency-Guidebook-Optimizing-Response-Times-for-GenAI-Applications/tree/main/notebooks-with-techniques \"notebooks-with-techniques\")| |   \n[.env_sample](https://github.com/Azure/The-LLM-Latency-Guidebook-Optimizing-Response-Times-for-GenAI-Applications/blob/main/.env_sample \".env_sample\")| [.env_sample](https://github.com/Azure/The-LLM-Latency-Guidebook-Optimizing-Response-Times-for-GenAI-Applications/blob/main/.env_sample \".env_sample\")| |   \n[.gitignore](https://github.com/Azure/The-LLM-Latency-Guidebook-Optimizing-Response-Times-for-GenAI-Applications/blob/main/.gitignore \".gitignore\")| [.gitignore](https://github.com/Azure/The-LLM-Latency-Guidebook-Optimizing-Response-Times-for-GenAI-Applications/blob/main/.gitignore \".gitignore\")| |   \n[CODE_OF_CONDUCT.md](https://github.com/Azure/The-LLM-Latency-Guidebook-Optimizing-Response-Times-for-GenAI-Applications/blob/main/CODE_OF_CONDUCT.md \"CODE_OF_CONDUCT.md\")| [CODE_OF_CONDUCT.md](https://github.com/Azure/The-LLM-Latency-Guidebook-Optimizing-Response-Times-for-GenAI-Applications/blob/main/CODE_OF_CONDUCT.md \"CODE_OF_CONDUCT.md\")| |   \n[LICENSE](https://github.com/Azure/The-LLM-Latency-Guidebook-Optimizing-Response-Times-for-GenAI-Applications/blob/main/LICENSE \"LICENSE\")| [LICENSE](https://github.com/Azure/The-LLM-Latency-Guidebook-Optimizing-Response-Times-for-GenAI-Applications/blob/main/LICENSE \"LICENSE\")| |   \n[README.md](https://github.com/Azure/The-LLM-Latency-Guidebook-Optimizing-Response-Times-for-GenAI-Applications/blob/main/README.md \"README.md\")| [README.md](https://github.com/Azure/The-LLM-Latency-Guidebook-Optimizing-Response-Times-for-GenAI-Applications/blob/main/README.md \"README.md\")| |   \n[SECURITY.md](https://github.com/Azure/The-LLM-Latency-Guidebook-Optimizing-Response-Times-for-GenAI-Applications/blob/main/SECURITY.md \"SECURITY.md\")| [SECURITY.md](https://github.com/Azure/The-LLM-Latency-Guidebook-Optimizing-Response-Times-for-GenAI-Applications/blob/main/SECURITY.md \"SECURITY.md\")| |   \n[SUPPORT.md](https://github.com/Azure/The-LLM-Latency-Guidebook-Optimizing-Response-Times-for-GenAI-Applications/blob/main/SUPPORT.md \"SUPPORT.md\")| [SUPPORT.md](https://github.com/Azure/The-LLM-Latency-Guidebook-Optimizing-Response-Times-for-GenAI-Applications/blob/main/SUPPORT.md \"SUPPORT.md\")| |   \n[pip_freeze_sample.txt](https://github.com/Azure/The-LLM-Latency-Guidebook-Optimizing-Response-Times-for-GenAI-Applications/blob/main/pip_freeze_sample.txt \"pip_freeze_sample.txt\")| [pip_freeze_sample.txt](https://github.com/Azure/The-LLM-Latency-Guidebook-Optimizing-Response-Times-for-GenAI-Applications/blob/main/pip_freeze_sample.txt \"pip_freeze_sample.txt\")| |   \n[requirements.txt](https://github.com/Azure/The-LLM-Latency-Guidebook-Optimizing-Response-Times-for-GenAI-Applications/blob/main/requirements.txt \"requirements.txt\")| [requirements.txt](https://github.com/Azure/The-LLM-Latency-Guidebook-Optimizing-Response-Times-for-GenAI-Applications/blob/main/requirements.txt \"requirements.txt\")| |   \nView all files  \n## Repository files navigation\n  * [README](https://github.com/Azure/The-LLM-Latency-Guidebook-Optimizing-Response-Times-for-GenAI-Applications)\n  * [Code of conduct](https://github.com/Azure/The-LLM-Latency-Guidebook-Optimizing-Response-Times-for-GenAI-Applications)\n  * [MIT license](https://github.com/Azure/The-LLM-Latency-Guidebook-Optimizing-Response-Times-for-GenAI-Applications)\n  * [Security](https://github.com/Azure/The-LLM-Latency-Guidebook-Optimizing-Response-Times-for-GenAI-Applications)\n\n\n# Optimizing GenAI Applications for Speed\n[](https://github.com/Azure/The-LLM-Latency-Guidebook-Optimizing-Response-Times-for-GenAI-Applications#optimizing-genai-applications-for-speed)\nGenerative AI applications are transforming how we do business today, creating new, engaging ways for customers to engage with applications. However, these new LLM models require massive amounts of compute to run, and unoptimized applications can run quite slowly, leading users to become frustrated. Creating a positive user experience is critical to the adoption of these tools, so minimising the response time of your LLM API calls is a must. The techniques shared in this article demonstrate how applications can be sped up by up to 100x their original speed through clever prompt engineering and a small amount of code!\nPrevious work has identified the core principles for reducing LLM response times. This article expands upon these, by providing practical examples coupled with working code, to help you accelerate your own applications and delight customers. This article is primarily intended for software developers, data scientists and application developers, though any business stakeholder managing GenAI applications should read on to learn new ideas for improving their customer experience.\n## Understanding the drivers of long response times\n[](https://github.com/Azure/The-LLM-Latency-Guidebook-Optimizing-Response-Times-for-GenAI-Applications#understanding-the-drivers-of-long-response-times)\nThe response time of an LLM can vary based on four primary factors:\n  1. The model used.\n  2. The number of tokens in the prompt.\n  3. The number of tokens generated.\n  4. The overall load on the deployment & system.\n\n\nYou can imagine the model as a person typing on a keyboard, where each token is generated one after another. The speed of the person (the model used) and the amount they need to type (the number of generation tokens) tend to be the largest contributor to long response times.\n_Figure 1 - The response generation step typically dominates the overall response time. Not to scale._\n## Techniques for improving LLM response times\n[](https://github.com/Azure/The-LLM-Latency-Guidebook-Optimizing-Response-Times-for-GenAI-Applications#techniques-for-improving-llm-response-times)\nThe below table contains a range of recommendations that can be implemented to improve the response times of your Generative AI application. Where applicable, sample code is included, to allow you to see these benefits for yourself, and copy the relevant code or prompts into your application.\nBest Practice | Intuition | GitHub | Potential Speed up of application  \n---|---|---|---  \n1. Generation Token Compression | Prompt the LLM to return the shortest response possible. A few simple phrases in your prompt can speed up your application. Few-shot prompting can also be used to ensure the response includes all the key information. | [Link](https://github.com/Azure/The-LLM-Latency-Guidebook-Optimizing-Response-Times-for-GenAI-Applications/blob/main/notebooks-with-techniques/generation-token-compression/generation-token-compression.ipynb) | Up to 2-3x or more20s â 8s  \n2. Avoid using LLMs to output large amounts of predetermined text | Rather than rewriting documents, use the LLM to identify which parts of the text need to be edited, and use code to make the edits. For RAG, use code to simply append documents to the LLM response. | [Link](https://github.com/Azure/The-LLM-Latency-Guidebook-Optimizing-Response-Times-for-GenAI-Applications/blob/main/notebooks-with-techniques/avoid-rewriting-documents/avoid-rewriting-documents.ipynb) | Up to 16x or more310s â 20s  \n3. Implement semantic caching | By caching responses, LLM responses can be reused, rather than calling Azure OpenAI, saving cost and time. The input does not need an exact match- for example âHow can I sign up for Azureâ and âI want to sign up for Azureâ will return the same cached result. | [Link](https://github.com/Azure/The-LLM-Latency-Guidebook-Optimizing-Response-Times-for-GenAI-Applications/blob/main/notebooks-with-techniques/semantic-caching/semantic-caching.ipynb) | Up to 14x or more19s â 1.3s  \n4. Parallelize requests | Many use cases (such as document processing, classification etc.) can be parallelized. | [Link](https://github.com/Azure/The-LLM-Latency-Guidebook-Optimizing-Response-Times-for-GenAI-Applications/blob/main/notebooks-with-techniques/parallelization/parallelization.ipynb) | Up to 72x or more180s â 2.5s  \n5. Use GPT-3.5 over GPT-4 where possible | GPT-3.5 has a much faster token generation speed. Certain use cases require the more advanced reasoning capabilities of GPT-4, however sometimes few-shot prompting or finetuning may enable GPT-3.5 to perform the same tasks. Generally only recommended for advanced users, after attempting other optimizations first. | [Link](https://github.com/Azure/The-LLM-Latency-Guidebook-Optimizing-Response-Times-for-GenAI-Applications/blob/main/notebooks-with-techniques/use-models-with-faster-time-between-tokens/use-models-with-faster-time-between-tokens.ipynb) | Up to 4x17s â 5s  \n6. Leverage translation services for certain languages | Certain languages have not been optimised, leading to long response times. Generate the output in English and leverage another model or API for the translation step. | [Link](https://github.com/Azure/The-LLM-Latency-Guidebook-Optimizing-Response-Times-for-GenAI-Applications/blob/main/notebooks-with-techniques/multilingual-optimization/multilingual-optimization.ipynb) | Up to 3x53s â 16s  \n7. Co-locate cloud resources | Ensure model is deployed close your users. Ensure Azure AI Search and Azure OpenAI are as closely located as possible (in the same region, firewall, vNet etc.). | NA | 1-2x  \n8. Load balancing | Having an additional endpoint for handling overflow capacity (for example, a PTU overflowing to a Pay-as-you-Go endpoint) can save latency by avoiding queuing when retrying requests. | [Link](https://github.com/Azure/The-LLM-Latency-Guidebook-Optimizing-Response-Times-for-GenAI-Applications/blob/main/notebooks-with-techniques/load-balancing/load-balancing.ipynb) | Up to 2x58s â 31s  \n## Putting it into practice through case studies\n[](https://github.com/Azure/The-LLM-Latency-Guidebook-Optimizing-Response-Times-for-GenAI-Applications#putting-it-into-practice-through-case-studies)\nThis section includes an overview of case studies that represent typical GenAI applicationsâperhaps one is similar to yours! The linked code repositories show the original speed of the application, and then walk you step-by-step through the process of implementing different combinations of the techniques in this document. Implementing these recommendations achieved an improvement in the response time ranging from 6.8-102x!\nCase Study | Techniques applied | Cumulative speed improvement | GitHub  \n---|---|---|---  \nDocument processingRewrite a document to correct spelling errors, grammar, and comply with an organizationâs style guide. | 1. Base case | 1x (315s) | [Link](https://github.com/Azure/The-LLM-Latency-Guidebook-Optimizing-Response-Times-for-GenAI-Applications/blob/main/case-studies/document-processing/document-processing.ipynb)  \n2. Avoid rewriting documents | 8.3x (38s)  \n3. Generation token compression | 15.8x (20s)  \n4. Parallelization | 105x (3s)  \nRetrieval Augmented Generation (RAG)Help a user troubleshoot a product which is not working. | 1. Base case | 1x (23s) | [Link](https://github.com/Azure/The-LLM-Latency-Guidebook-Optimizing-Response-Times-for-GenAI-Applications/blob/main/case-studies/retrieval-augmented-generation/retrieval-augmented-generation.ipynb)  \n2. Generation token compression | 2.3x (9.8s)  \n3. Avoid rewriting documents | 6.8x (3.4s)  \nRetrieval Augmented Generation (RAG)Provide general product information. | 1. Base case | 1x (17s) | [Link](https://github.com/Azure/The-LLM-Latency-Guidebook-Optimizing-Response-Times-for-GenAI-Applications/blob/main/case-studies/retrieval-augmented-generation/retrieval-augmented-generation.ipynb)  \n2. Semantic caching | 17x (1s)  \n## Conclusion\n[](https://github.com/Azure/The-LLM-Latency-Guidebook-Optimizing-Response-Times-for-GenAI-Applications#conclusion)\nWith Generative AI transforming how people interact with applications, minimising response times is essential. If youâre interested in improving your GenAI applicationâs performance, select a few of these recommendations, clone the repository, and implement them in your applicationâs next release!\n_Disclaimer: The results depicted are merely illustrative, emphasizing the potential benefits of these techniques. They are not all-encompassing and are based on a single test. Response times may differ with each run, thus the main goal is to demonstrate relative improvement. The tests are performed using the powerful, but slower, GPT-4 32k model, with a focus on improving response times. The effectiveness of techniques like error correction through document rewriting varies depending on the input; a document with many errors might take longer to correct than to rewrite entirely. Therefore, these techniques should be tailored to your application._\n## Contributing\n[](https://github.com/Azure/The-LLM-Latency-Guidebook-Optimizing-Response-Times-for-GenAI-Applications#contributing)\n**Thanks to the following key contributors:** Luca Stamatescu, Priya Kedia, Julian Lee, Manoranjan Rajguru, Shikha Agrawal, Michael Tremeer\nThis project welcomes contributions and suggestions. Most contributions require you to agree to a Contributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us the rights to use your contribution. For details, visit <https://cla.opensource.microsoft.com>.\nWhen you submit a pull request, a CLA bot will automatically determine whether you need to provide a CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions provided by the bot. You will only need to do this once across all repos using our CLA.\nThis project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/). For more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or contact opencode@microsoft.com with any additional questions or comments.\n## Trademarks\n[](https://github.com/Azure/The-LLM-Latency-Guidebook-Optimizing-Response-Times-for-GenAI-Applications#trademarks)\nThis project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft trademarks or logos is subject to and must follow [Microsoft's Trademark & Brand Guidelines](https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks/usage/general). Use of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship. Any use of third-party trademarks or logos are subject to those third-party's policies.\n## About\nThere are many articles that cover the principles of reducing latency optimization for LLMs, however it is often unclear how to actually implement these principles. This repository provides practical techniques for reducing the latency of GenAI applications. \n### Resources\n[ Readme ](https://github.com/Azure/The-LLM-Latency-Guidebook-Optimizing-Response-Times-for-GenAI-Applications#readme-ov-file)\n### License\n[ MIT license ](https://github.com/Azure/The-LLM-Latency-Guidebook-Optimizing-Response-Times-for-GenAI-Applications#MIT-1-ov-file)\n### Code of conduct\n[ Code of conduct ](https://github.com/Azure/The-LLM-Latency-Guidebook-Optimizing-Response-Times-for-GenAI-Applications#coc-ov-file)\n### Security policy\n[ Security policy ](https://github.com/Azure/The-LLM-Latency-Guidebook-Optimizing-Response-Times-for-GenAI-Applications#security-ov-file)\n###  Uh oh! \nThere was an error while loading. [Please reload this page](https://github.com/Azure/The-LLM-Latency-Guidebook-Optimizing-Response-Times-for-GenAI-Applications).\n[ Activity](https://github.com/Azure/The-LLM-Latency-Guidebook-Optimizing-Response-Times-for-GenAI-Applications/activity)\n[ Custom properties](https://github.com/Azure/The-LLM-Latency-Guidebook-Optimizing-Response-Times-for-GenAI-Applications/custom-properties)\n### Stars\n[ **27** stars](https://github.com/Azure/The-LLM-Latency-Guidebook-Optimizing-Response-Times-for-GenAI-Applications/stargazers)\n### Watchers\n[ **3** watching](https://github.com/Azure/The-LLM-Latency-Guidebook-Optimizing-Response-Times-for-GenAI-Applications/watchers)\n### Forks\n[ **8** forks](https://github.com/Azure/The-LLM-Latency-Guidebook-Optimizing-Response-Times-for-GenAI-Applications/forks)\n[ Report repository ](https://github.com/contact/report-content?content_url=https%3A%2F%2Fgithub.com%2FAzure%2FThe-LLM-Latency-Guidebook-Optimizing-Response-Times-for-GenAI-Applications&report=Azure+%28user%29)\n##  [Releases](https://github.com/Azure/The-LLM-Latency-Guidebook-Optimizing-Response-Times-for-GenAI-Applications/releases)\nNo releases published\n##  [Packages 0](https://github.com/orgs/Azure/packages?repo_name=The-LLM-Latency-Guidebook-Optimizing-Response-Times-for-GenAI-Applications)\nNo packages published \n###  Uh oh! \nThere was an error while loading. [Please reload this page](https://github.com/Azure/The-LLM-Latency-Guidebook-Optimizing-Response-Times-for-GenAI-Applications).\n##  [Contributors 2](https://github.com/Azure/The-LLM-Latency-Guidebook-Optimizing-Response-Times-for-GenAI-Applications/graphs/contributors)\n  * [ ![@microsoftopensource](https://avatars.githubusercontent.com/u/22527892?s=64&v=4) ](https://github.com/microsoftopensource) [ **microsoftopensource** Microsoft Open Source ](https://github.com/microsoftopensource)\n  * [ ![@luca-stamatescu](https://avatars.githubusercontent.com/u/52722879?s=64&v=4) ](https://github.com/luca-stamatescu) [ **luca-stamatescu** Luca Stamatescu ](https://github.com/luca-stamatescu)\n\n\n## Languages\n  * [ Jupyter Notebook 100.0% ](https://github.com/Azure/The-LLM-Latency-Guidebook-Optimizing-Response-Times-for-GenAI-Applications/search?l=jupyter-notebook)\n\n\n## Footer\n[ ](https://github.com) Â© 2025 GitHub, Inc. \n### Footer navigation\n  * [Terms](https://docs.github.com/site-policy/github-terms/github-terms-of-service)\n  * [Privacy](https://docs.github.com/site-policy/privacy-policies/github-privacy-statement)\n  * [Security](https://github.com/security)\n  * [Status](https://www.githubstatus.com/)\n  * [Docs](https://docs.github.com/)\n  * [Contact](https://support.github.com?tags=dotcom-footer)\n  * Manage cookies \n  * Do not share my personal information \n\n\nYou canât perform that action at this time. \n"
    }
]